# -*- coding: utf-8 -*-
"""Akhooli Gpt2  fine-tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tj14rItpS1x0xKO_MhbK0-BMX5zO7mYi
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import re

df = pd.read_csv('/content/drive/MyDrive/Arabic subtitles.xlsx - renewed.csv')

df

df = df.rename(columns={df.columns[0]: 'text'})

df = df[~df['text'].str.contains(r'\[[^\]]+\]')]

def clean_text(text):
    cleaned_text = re.sub(r'[^\u0600-\u06FF\s]', '', text)
    return cleaned_text

df['text'] = df['text'].apply(clean_text)

df = df[df['text'].str.strip() != '']

print(df)

from sklearn.model_selection import train_test_split

subs = df.text.copy()
subs

!pip install pyarrow requests==2.31.0

! pip install datasets transformers[torch] seqeval accelerate -U

import os
import time
import datetime

import pandas as pd
import seaborn as sns
import numpy as np
import random

import matplotlib.pyplot as plt

import torch
from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler
torch.manual_seed(42)

from transformers import AutoTokenizer, AutoModelWithLMHead, AutoConfig
from transformers import AdamW, get_linear_schedule_with_warmup

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("akhooli/gpt2-small-arabic")
model = AutoModelForCausalLM.from_pretrained("akhooli/gpt2-small-arabic")

print(tokenizer.special_tokens_map)

# Define special tokens
special_tokens_dict = {
    'bos_token': '<|startoftext|>',
    'eos_token': '<|endoftext|>',
    'unk_token': '<|unk|>',
    'pad_token': '<|pad|>'
}

# Add special tokens to the tokenizer
num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)

# Resize model embeddings to match the new tokenizer
model.resize_token_embeddings(len(tokenizer))

# Ensure pad_token_id is set in the model configuration
model.config.pad_token_id = tokenizer.pad_token_id

# Verify the special tokens
print(tokenizer.special_tokens_map)
print("Pad token ID:", tokenizer.pad_token_id)
print("EOS token ID:", tokenizer.eos_token_id)
print("BOS token ID:", tokenizer.bos_token_id)
print("UNK token ID:", tokenizer.unk_token_id)

batch_size = 2

class GPT2Dataset(Dataset):
    def __init__(self, txt_list, tokenizer, max_length=768):
        self.tokenizer = tokenizer
        self.input_ids = []
        self.attn_masks = []

        for txt in txt_list:
            encodings_dict = tokenizer(
                txt,
                truncation=True,
                max_length=max_length,
                padding="max_length",
                return_tensors="pt"
            )

            self.input_ids.append(encodings_dict['input_ids'].squeeze())
            self.attn_masks.append(encodings_dict['attention_mask'].squeeze())

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.attn_masks[idx]

dataset = GPT2Dataset(subs, tokenizer, max_length=540)

train_size = int(0.9 * len(dataset))
val_size = len(dataset) - train_size

train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

print('{:>5,} training samples'.format(train_size))
print('{:>5,} validation samples'.format(val_size))

train_dataloader = DataLoader(
            train_dataset,
            sampler = RandomSampler(train_dataset),
            batch_size = batch_size
        )

validation_dataloader = DataLoader(
            val_dataset,
            sampler = SequentialSampler(val_dataset),
            batch_size = batch_size
        )

# fine-tuning
configuration = AutoConfig.from_pretrained('akhooli/gpt2-small-arabic', output_hidden_states=False)

model = AutoModelWithLMHead.from_pretrained('akhooli/gpt2-small-arabic', config=configuration)

model.resize_token_embeddings(len(tokenizer))

device = torch.device("cuda")
model.cuda()

seed_val = 42

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

epochs = 3
learning_rate = 2e-5
warmup_steps = 1e2
epsilon = 1e-8

sample_every = 100

optimizer = AdamW(model.parameters(),
                  lr = learning_rate,
                  eps = epsilon
                )

total_steps = len(train_dataloader) * epochs

scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps = warmup_steps,
                                            num_training_steps = total_steps)

def format_time(elapsed):
    return str(datetime.timedelta(seconds=int(round((elapsed)))))

total_t0 = time.time()
training_stats = []

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

for epoch_i in range(epochs):
    print(f"\n======== Epoch {epoch_i + 1} / {epochs} ========")
    print("Training...")

    t0 = time.time()
    total_train_loss = 0

    model.train()

    for step, batch in enumerate(train_dataloader):
        b_input_ids = batch[0].to(device)
        b_labels = batch[0].to(device)
        b_masks = batch[1].to(device)

        model.zero_grad()

        outputs = model(
            b_input_ids,
            labels=b_labels,
            attention_mask=b_masks,
            token_type_ids=None
        )

        loss = outputs.loss
        batch_loss = loss.item()
        total_train_loss += batch_loss

        loss.backward()
        optimizer.step()
        scheduler.step()

        if step % sample_every == 0 and step != 0:
            elapsed = format_time(time.time() - t0)
            print(f'  Batch {step:>5,}  of  {len(train_dataloader):>5,}. Loss: {batch_loss:.3f}.   Elapsed: {elapsed}.')

            model.eval()

            # Sample generation example
            sample_outputs = model.generate(
                input_ids=b_input_ids,
                attention_mask=b_masks,
                do_sample=True,
                top_k=50,
                max_new_tokens=50,
                top_p=0.95,
                num_return_sequences=1,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )

            for i, sample_output in enumerate(sample_outputs):
                print(f"{i}: {tokenizer.decode(sample_output, skip_special_tokens=True)}")

            model.train()

    avg_train_loss = total_train_loss / len(train_dataloader)
    training_time = format_time(time.time() - t0)

    print(f"\n  Average training loss: {avg_train_loss:.2f}")
    print(f"  Training epoch took: {training_time}")

    print("\nRunning Validation...")
    t0 = time.time()

    model.eval()
    total_eval_loss = 0

    for batch in validation_dataloader:
        b_input_ids = batch[0].to(device)
        b_labels = batch[0].to(device)
        b_masks = batch[1].to(device)

        with torch.no_grad():
            outputs = model(
                b_input_ids,
                attention_mask=b_masks,
                labels=b_labels
            )

            loss = outputs.loss

        batch_loss = loss.item()
        total_eval_loss += batch_loss

    avg_val_loss = total_eval_loss / len(validation_dataloader)
    validation_time = format_time(time.time() - t0)

    print(f"  Validation Loss: {avg_val_loss:.2f}")
    print(f"  Validation took: {validation_time}")

    training_stats.append({
        'epoch': epoch_i + 1,
        'Training Loss': avg_train_loss,
        'Valid. Loss': avg_val_loss,
        'Training Time': training_time,
        'Validation Time': validation_time
    })

print("\nTraining complete!")
print(f"Total training took {format_time(time.time() - total_t0)} (h:mm:ss)")

df_stats = pd.DataFrame(data=training_stats)
df_stats = df_stats.set_index('epoch')
df_stats

# Use plot styling from seaborn.
sns.set(style='darkgrid')

# Increase the plot size and font size.
sns.set(font_scale=1.5)
plt.rcParams["figure.figsize"] = (12,6)

# Plot the learning curve.
plt.plot(df_stats['Training Loss'], 'b-o', label="Training")
plt.plot(df_stats['Valid. Loss'], 'g-o', label="Validation")

# Label the plot.
plt.title("Training & Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.xticks([1, 2, 3, 4])

plt.show()

params = list(model.named_parameters())

print('The GPT-2 model has {:} different named parameters.\n'.format(len(params)))

print('==== Embedding Layer ====\n')

for p in params[0:2]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

print('\n==== First Transformer ====\n')

for p in params[2:14]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

print('\n==== Output Layer ====\n')

for p in params[-2:]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

from google.colab import drive
import os
from transformers import AutoModel, AutoTokenizer

drive.mount('/content/drive')

output_dir = '/content/drive/My Drive/model_save/'

if not os.path.exists(output_dir):
    os.makedirs(output_dir)

print(f"Saving model to {output_dir}")

model_to_save = model.module if hasattr(model, 'module') else model
model_to_save.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

model.eval()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

prompt = "السلام "

generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)
generated = generated.to(device)

print(generated)

sample_outputs = model.generate(
                input_ids=b_input_ids,
                attention_mask=b_masks,
                do_sample=True,
                top_k=50,
                max_new_tokens=50,
                top_p=0.95,
                num_return_sequences=1,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )

for i, sample_output in enumerate(sample_outputs):
  print("{}: {}\n\n".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))





